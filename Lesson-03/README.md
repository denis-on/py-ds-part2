## Библиотеки Python для Data Science: продолжение
### Урок 3. Построение модели классификации.

* ##### Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?

**average=micro**
     учитывет общее количество истинных положительных результатов, ложных отрицательных результатов и ложных положительных результатов (независимо от прогноза для каждой метки в наборе данных). Дает каждой паре выборка-класс равный вклад в общую метрику (за исключением результата взвешивания выборки). Вместо того, чтобы суммировать метрику для каждого класса, он суммирует дивиденды и делители, составляющие метрики для каждого класса, для расчета общего частного. Микро-усреднение может быть предпочтительным в настройках с несколькими ярлыками, включая многоклассовую классификацию, когда класс большинства следует игнорировать.
**average=macro**
     для каждой метки и возвращает среднее значение без учета пропорции для каждой метки в наборе данных. просто вычисляет среднее значение двоичных показателей.
**average=weighted**
     для каждой метки и возвращает среднее значение с учетом пропорции для каждой метки в наборе данных. учитывает дисбаланс классов, вычисляя среднее значение двоичных показателей, в которых оценка каждого класса взвешивается по его присутствию в истинной выборке данных.
**average=samples**
     для каждого экземпляра и возвращает среднее значение. Используйте его для многоуровневой классификации. Он не вычисляет меру для каждого класса, вместо этого вычисляет метрику по истинным и прогнозируемым классам для каждой выборки в данных оценки и возвращает их  среднее значение



* ##### В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?

Алгоритмы отличаются друг от друга реализацией алгоритма boosted trees, а также технической совместимостью и ограничениями. XGBoost был первым, кто попытался улучшить время обучения GBM, за ним последовали LightGBM и CatBoost, каждый со своей собственной техникой, в основном связанной с механизмом расщепления.

**CatBoost** обладает гибкостью, позволяя задавать индексы категориальных столбцов, чтобы его можно было кодировать как кодирование в одно касание с использованием one_hot_max_size (используйте кодирование в одно касание для всех функций с числом различных значений, меньшим или равным данному значению параметра).

**LightGBM** использует новую технику односторонней выборки на основе градиента (GOSS) для фильтрации экземпляров данных для нахождения значения разделения, в то время как XGBoost использует предварительно отсортированный алгоритм и алгоритм на основе гистограммы для вычисления наилучшего разделения.Как и в CatBoost, LightGBM также может обрабатывать категориальные функции, вводя имена функций. Он не конвертируется в одноразовое кодирование и намного быстрее, чем одноразовое кодирование. LGBM использует специальный алгоритм, чтобы найти значение разделения категориальных признаков

**XGBoost** В отличие от CatBoost или LGBM, XGBoost не может обрабатывать категориальные функции сам по себе, он принимает только числовые значения, подобные случайному лесу. Поэтому перед подачей категориальных данных в XGBoost необходимо выполнить различные кодировки, такие как кодирование меток, среднее кодирование или однократное кодирование.

> К сожалению, не существует победителя по всем критериям. Поэтому например, если скорость обучения является нашей болевой точкой, мы используем LightGBM. В задачах, где нам нужна максимальная точность, мы можем обучить все алгоритмы, чтобы найти тот, у которого самый высокий WAUC для данного конкретного набора данных. 